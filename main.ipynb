{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR code generation using Concrete ML by Horaizon27 team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concrete import fhe\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "\n",
    "import training_utils\n",
    "import qrcode_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 4000\n",
    "QRCODE_VERSION = 1\n",
    "QRCODE_IMAGE_SIZE = 17 + QRCODE_VERSION * 4\n",
    "STYLE_NAME = \"green_orange\"\n",
    "TRAINING_QRCODES_DIR = Path(f\"{STYLE_NAME}-train_data\")\n",
    "DEFAULT_QRCODES_DIR = TRAINING_QRCODES_DIR / \"default\"\n",
    "STYLED_QRCODES_DIR = TRAINING_QRCODES_DIR / \"styled\"\n",
    "INFERENCE_RESULT_ROOT = Path(\"inference_result\")\n",
    "DEFAULT_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, default_qrcode_test, _ = \\\n",
    "    training_utils.create_qrcodes_datasets(DEFAULT_QRCODES_DIR, DATASET_SIZE)\n",
    "print (default_qrcode_test.shape)\n",
    "\n",
    "_, styled_qrcode_test, _ = \\\n",
    "    training_utils.create_qrcodes_datasets(STYLED_QRCODES_DIR, DATASET_SIZE)\n",
    "print (styled_qrcode_test.shape,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We are going to load and compile QuantAE and QuantAEPruned models\n",
    "- Run **model_training.ipynb** to train models\n",
    "- Make sure that model files are located in **ae_{QRCODE_IMAGE_SIZE}** and **ae_{QRCODE_IMAGE_SIZE}_pruned** directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = training_utils.load_model(f\"ae_{QRCODE_IMAGE_SIZE}\", QRCODE_IMAGE_SIZE)\n",
    "ae_model.to(device=DEFAULT_DEVICE).eval()\n",
    "\n",
    "pruned_ae_model = training_utils.load_model(\n",
    "    f\"ae_{QRCODE_IMAGE_SIZE}_pruned\", QRCODE_IMAGE_SIZE, pruned=True\n",
    ")\n",
    "pruned_ae_model.to(device=DEFAULT_DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputset(image_size):\n",
    "    inputset = np.ones([1, 1, image_size, image_size])\n",
    "    inputset[0][0][:image_size // 2][image_size // 2:] = 0\n",
    "    return inputset\n",
    "\n",
    "def get_compiled_model(model, inputset, n_bits=8):\n",
    "    compile_cfg = fhe.compilation.configuration.Configuration(\n",
    "        use_gpu=False, enable_unsafe_features=True, \n",
    "        parameter_selection_strategy=fhe.ParameterSelectionStrategy.MULTI\n",
    "    ) \n",
    "    return compile_brevitas_qat_model(\n",
    "        torch_model=model,\n",
    "        torch_inputset=inputset,\n",
    "        n_bits=n_bits,\n",
    "        rounding_threshold_bits={\"n_bits\": n_bits, \"method\": \"approximate\"},\n",
    "        configuration=compile_cfg,\n",
    "        p_error=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputset = generate_inputset(QRCODE_IMAGE_SIZE)\n",
    "\n",
    "fhe_ae_model = get_compiled_model(ae_model, inputset)\n",
    "fhe_pruned_ae_model = get_compiled_model(pruned_ae_model, inputset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Transfer comparison\n",
    "\n",
    "For comparison, we will try five options:\n",
    "1) QuantAE model (non FHE)\n",
    "2) Compiled QuantAE model in \"simulate mode\"\n",
    "3) Compiled QuantAE model in \"execute mode\"\n",
    "4) Compiled QuantAEPruned model in \"simulate\" mode\n",
    "5) Compiled QuantAEPruned model in \"execute mode\"\n",
    "\n",
    "We gonna measure **inference time**, **readability** and **reference diff**\n",
    "\n",
    "More information about models you can find in **README.md**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_transfer_stats(\n",
    "        model, default_qrcode_np_arrays, styled_qrcode_np_arrays, mode, debug=False\n",
    "):\n",
    "    inference_time = []\n",
    "    inference_image_diff = []\n",
    "    readable_qrcodes = 0\n",
    "    test_dataset_size = len(default_qrcode_np_arrays)\n",
    "\n",
    "    if debug:\n",
    "        results_dir = Path(\"inference_result\")\n",
    "        results_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for i in range(1, test_dataset_size + 1):\n",
    "        default_qrcode_array = np.float32(default_qrcode_np_arrays[i - 1])\n",
    "\n",
    "        if mode == \"non-fhe\":\n",
    "            test_image_tensor = qrcode_utils.np_qrcode_array_to_tensor(\n",
    "                default_qrcode_array\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            st_image_tensor = model(test_image_tensor)\n",
    "            inference_time.append(time.time() - start_time)\n",
    "            st_image_np_array = qrcode_utils.tensor_to_np_qrcode_array(st_image_tensor)\n",
    "        else:\n",
    "            test_image_ts_array = qrcode_utils.np_to_ts_qrcode_array(\n",
    "                default_qrcode_array\n",
    "            )\n",
    "            start_time = time.time()\n",
    "            st_image_ts_array = model.forward(test_image_ts_array, fhe=mode)\n",
    "            inference_time.append(time.time() - start_time)\n",
    "            st_image_np_array = qrcode_utils.ts_to_np_qrcode_array(st_image_ts_array)\n",
    "\n",
    "\n",
    "        inference_image_diff.append(\n",
    "            qrcode_utils.get_diff_between_image_arrays(st_image_np_array, styled_qrcode_np_arrays[i - 1])\n",
    "        )\n",
    "\n",
    "        corrected_st_image_array = qrcode_utils.get_corrected_qrcode_image(\n",
    "\t\t\tst_image_np_array, default_qrcode_array * 255\n",
    "\t\t)\n",
    "        if qrcode_utils.is_valid_qrcode(corrected_st_image_array):\n",
    "            readable_qrcodes += 1\n",
    "        \n",
    "        if debug:\n",
    "            qrcode_utils.qrcode_array_to_image(st_image_np_array).save(f\"{results_dir}/styled_{i}.jpg\")\n",
    "            qrcode_utils.qrcode_array_to_image(corrected_st_image_array).save(f\"{results_dir}/corrected_{i}.jpg\")\n",
    "\n",
    "    return {\n",
    "        \"avg_inference_time\": sum(inference_time) / len(inference_time),\n",
    "        \"qrcode_readability\": readable_qrcodes / test_dataset_size,\n",
    "        \"reference_diff\": np.mean(inference_image_diff)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-FHE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model.eval()\n",
    "ae_results = get_style_transfer_stats(\n",
    "    ae_model, default_qrcode_test, styled_qrcode_test, mode=\"non-fhe\", debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhe_ae_simulate_results = get_style_transfer_stats(\n",
    "    fhe_ae_model, default_qrcode_test, styled_qrcode_test, \n",
    "    mode=\"simulate\", debug=False\n",
    ")\n",
    "\n",
    "fhe_ae_pruned_simulate_results = get_style_transfer_stats(\n",
    "    fhe_pruned_ae_model, default_qrcode_test, styled_qrcode_test, \n",
    "    mode=\"simulate\", debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhe_ae_execute_results = get_style_transfer_stats(\n",
    "    fhe_ae_model, default_qrcode_test, styled_qrcode_test, \n",
    "    mode=\"execute\", debug=False\n",
    ")\n",
    "\n",
    "fhe_ae_pruned_execute_results = get_style_transfer_stats(\n",
    "    fhe_pruned_ae_model, default_qrcode_test, styled_qrcode_test, \n",
    "    mode=\"execute\", debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            ae_results[fileid],\n",
    "            fhe_ae_simulate_results[fileid],\n",
    "            fhe_ae_pruned_simulate_results[fileid],\n",
    "            fhe_ae_execute_results[fileid],\n",
    "            fhe_ae_pruned_execute_results[fileid]\n",
    "        ]\n",
    "        for fileid in [\"avg_inference_time\", \"qrcode_readability\", \"reference_diff\"]\n",
    "    ], \n",
    "    index=[\"Inference time\", \"Readability\", \"Reference diff\"], \n",
    "    columns=[\"QuantAE (Non-FHE)\", \"QuantAE (Sim)\", \"QuantAEPruned (Sim)\", \"QuantAE (FHE)\", \"QuantAEPruned (FHE)\"]\n",
    ")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
