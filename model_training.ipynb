{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import training_utils \n",
    "import qrcode_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = 4000\n",
    "\n",
    "QRCODE_VERSION = 1\n",
    "QRCODE_IMAGE_SIZE = 17 + QRCODE_VERSION * 4\n",
    "STYLE_NAME = \"green_orange\"\n",
    "TRAINING_QRCODES_DIR = Path(f\"{STYLE_NAME}-train_data\")\n",
    "DEFAULT_QRCODES_DIR = TRAINING_QRCODES_DIR / \"default\"\n",
    "STYLED_QRCODES_DIR = TRAINING_QRCODES_DIR / \"styled\"\n",
    "DEFAULT_DEVICE = \"cuda\" # \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training QR codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_data():\n",
    "    return ''.join(\n",
    "        random.choices(string.ascii_letters, k = random.randint(1, 10))\n",
    "    )\n",
    "\n",
    "def generate_training_qrcodes(\n",
    "        style_name, dataset_size, qrcode_version, \n",
    "        default_qrcodes_path, styled_qrcodes_path, force=False\n",
    "    ):\n",
    "    if not force and default_qrcodes_path.exists():\n",
    "        return\n",
    "    \n",
    "    default_qrcodes_path.mkdir(exist_ok=True, parents=True)\n",
    "    styled_qrcodes_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for i in range(1, dataset_size + 1):\n",
    "        qrcode_data = generate_random_data()\n",
    "\n",
    "        default_qrcode_img = qrcode_utils.generate_qrcode_image(\n",
    "            qrcode_version, qrcode_data\n",
    "        )\n",
    "        default_qrcode_img.save(default_qrcodes_path / f\"{i}.jpg\")\n",
    "\n",
    "        styled_qrcode_img = qrcode_utils.generate_qrcode_image(\n",
    "            qrcode_version, qrcode_data, qrcode_utils.get_color_mask(style_name)\n",
    "        )\n",
    "        styled_qrcode_img.save(styled_qrcodes_path / f\"{i}.jpg\")\n",
    "\n",
    "generate_training_qrcodes(\n",
    "    STYLE_NAME, DATASET_SIZE, QRCODE_VERSION, \n",
    "    DEFAULT_QRCODES_DIR, STYLED_QRCODES_DIR, force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_qrcode_train, _, default_qrcode_val = \\\n",
    "    training_utils.create_qrcodes_datasets(DEFAULT_QRCODES_DIR, DATASET_SIZE)\n",
    "print (default_qrcode_train.shape, default_qrcode_val.shape)\n",
    "\n",
    "st_qrcode_train, _, st_qrcode_val = \\\n",
    "    training_utils.create_qrcodes_datasets(STYLED_QRCODES_DIR, DATASET_SIZE)\n",
    "print (st_qrcode_train.shape, st_qrcode_val.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    training_utils.QRCodeImageDataset(\n",
    "        default_qrcode_train, st_qrcode_train, QRCODE_IMAGE_SIZE\n",
    "    ),\n",
    "    batch_size=5, shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    training_utils.QRCodeImageDataset(\n",
    "        default_qrcode_val, st_qrcode_val, QRCODE_IMAGE_SIZE\n",
    "    ),\n",
    "    batch_size=5, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We are going to train two models: QuantAE and QuantAEPruned\n",
    "- QuantAEPruned is just a prunned variation of QuantAE\n",
    "- Both models will use the same generated training data\n",
    "\n",
    "More information about models you can find in README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_model = training_utils.get_ae_model(f\"ae_{QRCODE_IMAGE_SIZE}\", QRCODE_IMAGE_SIZE)\n",
    "ae_model.to(device=DEFAULT_DEVICE)\n",
    "\n",
    "pruned_ae_model = training_utils.get_ae_pruned_model(f\"ae_{QRCODE_IMAGE_SIZE}_pruned\", QRCODE_IMAGE_SIZE)\n",
    "pruned_ae_model.to(device=DEFAULT_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "- 3 stages, each consists of 30 epoch\n",
    "- Learning rate is set to default before each stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PARAMS = {\n",
    "    \"LR\": 0.001,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"scheduler_gamma\": 0.95,\n",
    "    \"device\": DEFAULT_DEVICE,\n",
    "    \"stages\": {\n",
    "        1: {\n",
    "            \"start_epoch\": 0,\n",
    "            \"epochs\": 30\n",
    "        },\n",
    "        2: {\n",
    "            \"start_epoch\": 30,\n",
    "            \"epochs\": 60\n",
    "        },\n",
    "        3: {\n",
    "            \"start_epoch\": 60,\n",
    "            \"epochs\": 90\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model with three stages of training\n",
    "for stage in [1, 2, 3]:\n",
    "    training_utils.train(\n",
    "        model = ae_model,\n",
    "        dataloader_train = train_loader,\n",
    "        dataloader_val = val_loader,\n",
    "        stage = stage,\n",
    "        params = TRAINING_PARAMS,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruned AE  model with three stages of training\n",
    "pruned_ae_model.prune(True)\n",
    "for stage in [1, 2, 3]:\n",
    "    training_utils.train(\n",
    "        model = pruned_ae_model,\n",
    "        dataloader_train = train_loader,\n",
    "        dataloader_val = val_loader,\n",
    "        stage = stage,\n",
    "        params = TRAINING_PARAMS,\n",
    "    )\n",
    "pruned_ae_model.prune(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_utils.save_model(ae_model)\n",
    "training_utils.save_model(pruned_ae_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
